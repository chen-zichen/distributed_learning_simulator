2021-04-06 13:12:01,844 INFO {thd:140589208753984} [arg_parse.py => 72] : use dataset MNIST and model LeNet5
2021-04-06 13:12:17,009 INFO {thd:140586666145344} [trainer.py => 144] : training_set_size is 49995
2021-04-06 13:12:17,010 INFO {thd:140586666145344} [trainer.py => 148] : use device cuda:0
2021-04-06 13:12:17,010 INFO {thd:140589208753984} [simulator.py => 52] : begin training
2021-04-06 13:12:18,778 INFO {thd:140586666145344} [trainer.py => 359] : begin training, hyper_parameter is epochs:1 batch_size:64 learning_rate:0.01 weight_decay:1 optimizer:<class 'torch.optim.sgd.SGD'>, optimizer is SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 2.0002000200020003e-05
) ,lr_scheduler is <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdcfdcf0bd0>, model: LeNet5, loss_fun: NLLLoss(), parameter number is 61706
2021-04-06 13:12:19,040 ERROR {thd:140587127633472} [task_queue.py => 49] : catch exception:name 'function_gradient' is not defined
2021-04-06 13:12:19,046 ERROR {thd:140587127633472} [task_queue.py => 50] : traceback:Traceback (most recent call last):
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_lib-0.1-py3.7.egg/cyy_naive_lib/data_structure/task_queue.py", line 36, in worker
    res = worker_fun(task, extra_arguments)
  File "/home/cicie/distributed_learning_simulator/qsgd/qsgd_server.py", line 47, in __worker
    total_q_gradients[idx] = function_gradient(q_gradients)
NameError: name 'function_gradient' is not defined

