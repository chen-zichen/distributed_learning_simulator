2021-04-06 12:58:50,023 INFO {thd:139827165660992} [arg_parse.py => 72] : use dataset MNIST and model LeNet5
2021-04-06 12:59:17,620 INFO {thd:139824559433280} [trainer.py => 144] : training_set_size is 24994
2021-04-06 12:59:17,620 INFO {thd:139824559433280} [trainer.py => 148] : use device cuda:0
2021-04-06 12:59:17,684 INFO {thd:139823978591808} [trainer.py => 144] : training_set_size is 25001
2021-04-06 12:59:17,684 INFO {thd:139823978591808} [trainer.py => 148] : use device cuda:1
2021-04-06 12:59:17,684 INFO {thd:139827165660992} [simulator.py => 52] : begin training
2021-04-06 12:59:20,588 INFO {thd:139824559433280} [trainer.py => 359] : begin training, hyper_parameter is epochs:2 batch_size:64 learning_rate:0.01 weight_decay:1 optimizer:<class 'torch.optim.sgd.SGD'>, optimizer is SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 4.000960230455309e-05
) ,lr_scheduler is <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2b847df9d0>, model: LeNet5, loss_fun: NLLLoss(), parameter number is 61706
2021-04-06 12:59:20,616 INFO {thd:139823978591808} [trainer.py => 359] : begin training, hyper_parameter is epochs:2 batch_size:64 learning_rate:0.01 weight_decay:1 optimizer:<class 'torch.optim.sgd.SGD'>, optimizer is SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 3.999840006399744e-05
) ,lr_scheduler is <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2b847d0dd0>, model: LeNet5, loss_fun: NLLLoss(), parameter number is 61706
2021-04-06 12:59:20,967 ERROR {thd:139824559433280} [executor_pool.py => 37] : catch exception:function_gradient() missing 1 required positional argument: 'q_gradients'
2021-04-06 12:59:20,971 ERROR {thd:139823978591808} [executor_pool.py => 37] : catch exception:function_gradient() missing 1 required positional argument: 'q_gradients'
2021-04-06 12:59:20,974 ERROR {thd:139824559433280} [executor_pool.py => 38] : traceback:Traceback (most recent call last):
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_lib-0.1-py3.7.egg/cyy_naive_lib/data_structure/executor_pool.py", line 35, in process
    fn(*args, **kwargs)
  File "/home/cicie/distributed_learning_simulator/qsgd/qsgd_worker.py", line 18, in train
    device=device, optimizer_step_callbacks=[self.__get_gredient]
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_pytorch_lib-0.1-py3.7.egg/cyy_naive_pytorch_lib/trainer.py", line 482, in train
    return super().train(**kwargs)
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_pytorch_lib-0.1-py3.7.egg/cyy_naive_pytorch_lib/trainer.py", line 405, in train
    return super().train(**kwargs)
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_pytorch_lib-0.1-py3.7.egg/cyy_naive_pytorch_lib/trainer.py", line 253, in train
    callback(optimizer, trainer=self, device=device)
  File "/home/cicie/miniconda/envs/fedml/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/home/cicie/distributed_learning_simulator/qsgd/qsgd_worker.py", line 48, in __get_gredient
    d_p = QsgdServer.function_gradient(d_p).detach().cpu()
TypeError: function_gradient() missing 1 required positional argument: 'q_gradients'

2021-04-06 12:59:20,976 ERROR {thd:139823978591808} [executor_pool.py => 38] : traceback:Traceback (most recent call last):
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_lib-0.1-py3.7.egg/cyy_naive_lib/data_structure/executor_pool.py", line 35, in process
    fn(*args, **kwargs)
  File "/home/cicie/distributed_learning_simulator/qsgd/qsgd_worker.py", line 18, in train
    device=device, optimizer_step_callbacks=[self.__get_gredient]
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_pytorch_lib-0.1-py3.7.egg/cyy_naive_pytorch_lib/trainer.py", line 482, in train
    return super().train(**kwargs)
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_pytorch_lib-0.1-py3.7.egg/cyy_naive_pytorch_lib/trainer.py", line 405, in train
    return super().train(**kwargs)
  File "/home/cicie/.local/lib/python3.7/site-packages/cyy_naive_pytorch_lib-0.1-py3.7.egg/cyy_naive_pytorch_lib/trainer.py", line 253, in train
    callback(optimizer, trainer=self, device=device)
  File "/home/cicie/miniconda/envs/fedml/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/home/cicie/distributed_learning_simulator/qsgd/qsgd_worker.py", line 48, in __get_gredient
    d_p = QsgdServer.function_gradient(d_p).detach().cpu()
TypeError: function_gradient() missing 1 required positional argument: 'q_gradients'

